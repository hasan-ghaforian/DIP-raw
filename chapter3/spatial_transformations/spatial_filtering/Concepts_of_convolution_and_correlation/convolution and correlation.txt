
\star

Conventions
    When we write A ★ B, we call A as "kernel" and "B" as image.

    For image processing the size of kernel must be odd.

Definition of "same size" and "extended" (full) filtering operation

    consider w ★ f, now we have two different approaches for obtaining
    convolution:

        same size:  center of kernel (w) visits every elment of f

        Extended:   We can deﬁne correlation and convolution so that every
        element of w (instead of just its center) visits every pixel in f.

        result of "same size" way is always a part of result of "extended" way.

Computing the effect of kernel on its underlying area
    Suppose the size of kernel is (2p+1)(2q+1). For each position where the
    kernel can be in, compute this:
        w ★ f (p,q) = \sigma\limits_{-p}^{p} \sigma\limits_{-q}^{q} w(p,q)
        f(x-p,y-q)
    We have to write the result of convolution in the center of the area which
    is under the kernel.

 
When have we use "same size" and when "extended"?
    If size of kernel is greater than image, we have to use "extended", else we
    can use "same size" way.

Size of result
    Although we add some padding to the image; but we do not consider all of its
    elements as result of convolution! We only consider the elements that the
    center of kernel had sit on it, even when result for that center be 0.
    
    So in "same size" way, although the size of padded image is
    size(f)+size(kernel)-1 but the size of result will be same as size of f
    itself. 

    In the extended way, althought the size of padded image is size(f) +
    2(size(kernel) - 1); but the center of kernel only can sit between
    (size(kernel) - 1)/2 and size(f) + (size(kernel) - 1)/2. So the size of
    result will be size(f) + size(kernel) -1.

    If we have this convolution:
        W ★ f
    and size of f be MN and size of W be mn, then 
        In same size, we have to start by putting center of kernel on element
        (1,1) of image (top-left pixel). So we have to padd image by adding
        (m-1)/2 rows above and same below of f. Also we have to add (n-1)/2
        elements to the right and also same number to the left of f.
        In the full convolution, we have to put the bottom right of kernel on
        the top right element of f. So we have to add padding with the size n-1
        to both left and right and m-1 to the top and below of f. So the size of
        padded image will be (M + 2(m-1)) (N + 2(n-1)).
    
    So for a full (extended) convolution, size of result will be:
        
        (M+m-1) (N+n-1)                              (🟔)
    
    For "same size" filtering, the size will be:

        (M + (m-1)/2) (N + (n-1)/2)                  (🟍)

    The same is correct for size of result for a full correlation.

Are A ★ B and B ★ A really equal?
    I think in discrete math, we do convolution by padding infinite 0s to the
    start and end of image, before calculating the convolution; and it is legal
    because limits of \sigma are ∞. For image processing, we add enough 0s with
    attention to the way (same size or extended) and as we saw earlier, the size
    may be different. Really when we say A ★ B and B ★ A are equal, we intended
    using extended way for evaluating both convolutions, regardless to the size
    of image and kernel. Note that the result of "same size" way is always part
    of extended way.

Number of multiply and sum operation for convolution
    Suppose in w ★ f, size of kernel (w) is mn and image is MN. Now number of
    operations deped on the real size of result and so depends on the way for
    doing convolution. Suppose size of result of convolution (not the padded
    image) be PQ. We know for each pixel in final result we must calculate mn
    operations (when it was the center of area under the kernel). So the total
    number of operations will be PQmn. For example for a "same size"
    convolution, the number is MNmn.

Chained convolution and correlations

    Sometimes an image is ﬁltered (i.e., convolved) sequentially, in stages,
    using a different kernel in each stage. For example, suppose than an image f
    is ﬁltered with a kernel w1 , the result ﬁltered with kernel w2 , that
    result ﬁltered with a third kernel, and so on, for Q stages.

    (W_Q ★ (W_{Q-1} ★ ... ★ (W_2 ★ (W_1 ★ f) ...))

    Because of the associative property of convolution we can write it:

    (W_Q ★ (W_{Q-1} ★ ... ★ (W_2 ★ (W_1 ★ f) ...)) = 
    (W_Q ★ (W_{Q-1} ★ ... ★ ((W_2 ★ W_1) ★ f) ...)) = 
    ...
    (W_Q ★ W_{Q-1} ★ ... ★ W_2 ★ W_1) ★ f

    Now because of commutative property of convolution, this multistage ﬁltering
    can be done in a single ﬁltering operation, w ★ f:

    (W_Q ★ W_{Q-1} ★ ... ★ W_2 ★ W_1) ★ f = 
    (W_1 ★ W_2 ★ ... ★ W_{Q-1} ★ W_Q) ★ f 

Size of chained filters
    In this case, we have to use relation (🟔) or (🟍) for determining that.
    For example if we have Q filters and size of all of them be mn, then size of
    equal filter W will be:
    ((Q-1) × (m − 1) + m) ((Q-1) × (n − 1) + n)

    For a "same size" filtering with Q filters with size equal to mn, the size
    of equal filter will be:
    (m + (Q-1) × (m-1)/2) (n + (Q-1) × (m-1)/2)


Convolution and product of vectors
    Matrix product of a column vector with a row vector is equivalent to the 2-D
    convolution of the two vectors (full or extended convolution). The vectors
    do not have to be of the same length.
    Note: As this is about product of a column vector with a row vector, size of
    vectors may be not equal.

SEPARABLE FILTER KERNELS
    Definition
        From matrix theory we know the rank of producting a columnar and a row
        vectors will be 1. And if rank of matrix be 1, then it can be product by
        producting a columnar and a row vectors. If a matrix can be product in
        such way, we say it is "separable" matrix.
    
    Usage:
        Suppose W is a separable matrix:
            W = W_1 ★ W_2
        and W_1 is columnar. Now for W ★ f we have:
            W ★ f = W_1 ★ W_2 ★ f = (W_1 ★ f) ★ W_2
        This approach reduces the number of products and sums needed for doing
        convolution. Suppose f is MN and W is mn. For calculating convolution in
        usuall way, we have to do MNmn operations. But as W_1 has m rows, the 
        W_1 ★ f needs MNm operations and for "same size" convolution, size of
        result will be again MN. 
        ? >> Now (W_1 ★ f) ★ W_2 will need to MNn operations. So we did the
        convolution with MN(m+n) operations.
        I guess >> Now W_2 ★ (W_1 ★ f) will need to MNn
        operations. 
        So we did the convolution with MN(m+n) operations instead of MNmn
        operations.
    
    Separating
        Only if the rank of matrix is 1, we can separate it.

        1. Find any nonzero element in the kernel and let E denote its value.
        2. Form vectors c and r equal, respectively, to the column and row in
        the kernel containing the element found in Step 1.
        3. With reference to Eq. (3-41), let v = c and w^T = r/E

        Pay attention to these:
            not 0 pivot
            divide the row by pivot

        For a square kernel, the result of separating will be VV^T

        Note: When I saw the (W_1 ★ f) ★ W_2, I thoughtfor doing convolution by
        separating the kernel, we use "same size" way. for doing (W_1 ★ f) ★
        W_2; because the number of operations will be MNn. But if you do
        (consider W_1 ★ f as kernel and W_2 as image), you will lose information
        of image; obviously size of result will be n while size of original
        image was MN. If you try extended way for doing (W_1 ★ f) ★ W_2, you
        will not lose data as itss result will have M(N+n-1) elements and the
        current filter (result of W_1 ★ f) has MN elements, only this step needs
        to MN(M(N+n-1)) operations!
        So I think we use same size method for W_1 ★ f and then again same size
        method for W_2 ★ (W_1 ★ f) (it means we use W_2 as kernel for second step).
        Also with attention to the associative and commutative properties of
        convolution we can write:
            W ★ f = W_1 ★ W_2 ★ f = (W_1 ★ f) ★ W_2 = W_2 ★ (W_1 ★ f)
        So finally I think we have to use at first W_1 as kernel and then W_2 as
        kernel for result of step one. You can see the same process here
        (https://blogs.mathworks.com/steve/2006/10/04/separable-convolution/)
        while writer says:
            then you can filter with s by filtering first with v, and then
            filtering the result with h.
        
        ATTENTION:
            For associative and commutative properties are only legal for
            convolution. I think for doing a correlation, you have to rotate it
            180 degree to get a convolution and then use those properties.

Relation between convolution and correlation
    https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10452/104520Y/Teaching-the-concept-of-convolution-and-correlation-using-Fourier-transform/10.1117/12.2267976.full?SSO=1

    WHY convolution is:
        commutative
        associative
    BUT correlation is not? AT LEAST for discrete case?


